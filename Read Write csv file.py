# Databricks notebook source
# MAGIC %python
# MAGIC storageAccountName = "adfdemostorage1"
# MAGIC storageAccountAccessKey = "FfjRxSDs8wrkGGOpgKHc0vnKzBoafTD1qSNcuNXD+ghcKjh3gbXNHhHXE4fJPbqBgAtz5QF5qNqO+AStNxqV0Q=="
# MAGIC sasToken = "?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-10-11T02:55:02Z&st=2023-10-06T18:55:02Z&spr=https&sig=3sQStZ86hBGx3c%2FPP8r0C633ArgOJGhybK5uZv3%2Fjco%3D"
# MAGIC blobContainerName = "databricksstorage"
# MAGIC mountPoint = "/mnt/data/"
# MAGIC if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):
# MAGIC   try:
# MAGIC     dbutils.fs.mount(
# MAGIC       source = "wasbs://{}@{}.blob.core.windows.net".format(blobContainerName, storageAccountName),
# MAGIC       mount_point = mountPoint,
# MAGIC       #extra_configs = {'fs.azure.account.key.' + storageAccountName + '.blob.core.windows.net': storageAccountAccessKey}
# MAGIC       extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}
# MAGIC     )
# MAGIC     print("mount succeeded!")
# MAGIC   except Exception as e:
# MAGIC     print("mount exception", e)

# COMMAND ----------

# MAGIC %python
# MAGIC dbutils.fs.mounts()

# COMMAND ----------

# MAGIC %md
# MAGIC Pyspark read & write CSV file refering to following link.              
# MAGIC https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/ 
# MAGIC downloaded the sample zipcode.csv file from https://github.com/spark-examples/pyspark-examples/blob/master/resources/zipcodes.csv
# MAGIC uploaded the zipcodes.csv file into datalake storage storageAccountName = "adfdemostorage1" & blobContainerName = "databricksstorage" which is mounted to dbfs path "/mnt/data"

# COMMAND ----------

df=spark.read.options(header='True',inferSchema='True').csv("/mnt/data/zipcodes.csv")
df.printSchema()
df.show()

# COMMAND ----------

#writing the dataframe to parquet file
df.write.format("parquet").save("/mnt/data/zipcodes")
#renaming the autogenerated file name to custom name
file_list=dbutils.fs.ls("/mnt/data/zipcodes")
for files in file_list:
    if files.name.endswith(".parquet"):
        filename=files.name
dbutils.fs.mv('/mnt/data/zipcodes/'+filename,'/mnt/data/zipcodes/zipcodes_file.parquet')

# COMMAND ----------

#creating dataframe from parquet file
df=spark.read.options(header="True",inferSchema="True",delimiter=",").format("parquet").load("/mnt/data/zipcodes/zipcodes_file.parquet")
df.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC Reading CSV files with a user-specified custom schema.

# COMMAND ----------

from pyspark.sql.types import StringType,IntegerType,StructType,StructField,DoubleType,BooleanType
schema=StructType([
StructField("RecordNumber",IntegerType(),True) 
,StructField("Zipcode",IntegerType(),True) 
,StructField("ZipCodeType",StringType(),True) 
,StructField("City",StringType(),True) 
,StructField("State",StringType(),True) 
,StructField("LocationType",StringType(),True) 
,StructField("Lat",DoubleType(),True) 
,StructField("Long",DoubleType(),True) 
,StructField("Xaxis",IntegerType(),True) 
,StructField("Yaxis",DoubleType(),True) 
,StructField("Zaxis",DoubleType(),True) 
,StructField("WorldRegion",StringType(),True) 
,StructField("Country",StringType(),True) 
,StructField("LocationText",StringType(),True) 
,StructField("Location",StringType(),True) 
,StructField("Decommisioned",BooleanType(),True) 
,StructField("TaxReturnsFiled",StringType(),True) 
,StructField("EstimatedPopulation",IntegerType(),True) 
,StructField("TotalWages",IntegerType(),True) 
,StructField("Notes",StringType(),True)
])
df=spark.read.option("delimiter",",").option("header","true").schema(schema).csv("/mnt/data/zipcodes.csv")
df.printSchema()
df.show(truncate=False)
